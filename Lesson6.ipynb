{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition on Kaggle: https://www.kaggle.com/c/how-good-is-your-medium-article/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.28.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_write(path_to_data,\n",
    "                               inp_filename, is_train=True):\n",
    "    ident = 0\n",
    "    features = ['_id', 'content', 'published', 'title', 'author']\n",
    "    prefix = 'train' if is_train else 'test'\n",
    "    feature_files = [open(os.path.join(path_to_data,\n",
    "                                       '{}_{}.txt'.format(prefix, feat)),\n",
    "                          'w', encoding=\"utf-8\")\n",
    "                     for feat in features]\n",
    " \n",
    "    with open(os.path.join(path_to_data, inp_filename), \n",
    "              encoding='utf-8') as inp_json_file:\n",
    "    \n",
    "        for line in tqdm_notebook(inp_json_file):\n",
    "            dict_data = {}\n",
    "            json_data = read_json_line(line)\n",
    "            for i in range(len(feature_files)):\n",
    "                dict_data[ident] = json_data[features[i]]\n",
    "                feature_files[i].write(str(dict_data) + '\\n')\n",
    "            ident += 1\n",
    "    for i in feature_files:\n",
    "        i.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'C:\\mlcourse\\Databases\\kaggle_medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf9d0159bac49febd94f3746ee4faf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_features_and_write(PATH_TO_DATA, 'train.json', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a7c87a45aa445295c5afd15ec214fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_features_and_write(PATH_TO_DATA, 'test.json', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the following groups of features:\n",
    "\n",
    "- Tf-Idf with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "- Tf-Idf with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "- Time features: publication hour, whether it's morning, day, night, whether it's a weekend\n",
    "- Bag of authors (i.e. One-Hot-Encoded author names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extacting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['_id', 'content', 'published', 'title', 'author']\n",
    "\n",
    "train_files = [os.path.join(PATH_TO_DATA,\n",
    "                                       '{}_{}.txt'.format('train', feat))   \n",
    "                     for feat in features]\n",
    "\n",
    "test_files = [os.path.join(PATH_TO_DATA,\n",
    "                                       '{}_{}.txt'.format('test', feat))   \n",
    "                     for feat in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all_data = pd.DataFrame({})\n",
    "for k in range(0, len(features)):\n",
    "    f = codecs.open(train_files[k], \"r\", \"utf-8\")\n",
    "    arr = []\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        count += 1\n",
    "        arr.append(list(eval(line[:-2]).values())[0])\n",
    "    f.close()\n",
    "    ser_buf = pd.Series(arr)\n",
    "    df_all_data[features[k]] = ser_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all_test = pd.DataFrame({})\n",
    "for k in range(0, len(features)):\n",
    "    f = codecs.open(test_files[k], \"r\", \"utf-8\")\n",
    "    arr = []\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        count += 1\n",
    "        arr.append(list(eval(line[:-2]).values())[0])\n",
    "    f.close()\n",
    "    ser_buf = pd.Series(arr)\n",
    "    df_all_test[features[k]] = ser_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data['content'] = df_all_data['content'].map(strip_tags)\n",
    "df_all_test['content'] = df_all_test['content'].map(strip_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a popularity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:\\mlcourse\\Databases\\kaggle_medium/train_log1p_recommends.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data = df_all_data.join(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip content: \\xa0, \\u200b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddata = df_all_data.copy(deep=True)\n",
    "dddata_test = df_all_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data = dddata.copy(deep=True)\n",
    "df_all_test = dddata_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division publications by hour, whether it's morning, day, night, whether it's a weekend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>content</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>log_recommends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://medium.com/policy/medium-terms-of-serv...</td>\n",
       "      <td>Medium Everyone’s stories and ideas Aug 13, 20...</td>\n",
       "      <td>{'$date': '2012-08-13T22:54:53.510Z'}</td>\n",
       "      <td>Medium Terms of Service – Medium Policy – Medium</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@Med...</td>\n",
       "      <td>8</td>\n",
       "      <td>9.01201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://medium.com/policy/amendment-to-medium-...</td>\n",
       "      <td>Medium Everyone’s stories and ideas Aug 2, 201...</td>\n",
       "      <td>{'$date': '2015-08-03T07:44:50.331Z'}</td>\n",
       "      <td>Amendment to Medium Terms of Service Applicabl...</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@Med...</td>\n",
       "      <td>14</td>\n",
       "      <td>3.49651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://medium.com/@aelcenganda/%E9%96%A9%E6%9...</td>\n",
       "      <td>Yun-Chen Chien（簡韻真） Nobody in @g0v.tw, PM in s...</td>\n",
       "      <td>{'$date': '2017-02-05T13:08:17.410Z'}</td>\n",
       "      <td>走入山與海之間：閩東大刀會和兩岸走私 – Yun-Chen Chien（簡韻真） – Medium</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@ael...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.69315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://medium.com/what-comes-to-mind/how-fast...</td>\n",
       "      <td>Vaibhav Khulbe Android App Developer | I write...</td>\n",
       "      <td>{'$date': '2017-05-06T08:16:30.776Z'}</td>\n",
       "      <td>How fast can a camera get? – What comes to min...</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@vai...</td>\n",
       "      <td>22</td>\n",
       "      <td>1.38629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://medium.com/what-comes-to-mind/a-game-f...</td>\n",
       "      <td>Vaibhav Khulbe Android App Developer | I write...</td>\n",
       "      <td>{'$date': '2017-06-04T14:46:25.772Z'}</td>\n",
       "      <td>A game for the lonely fox – What comes to mind...</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@vai...</td>\n",
       "      <td>29</td>\n",
       "      <td>1.94591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id  \\\n",
       "0  https://medium.com/policy/medium-terms-of-serv...   \n",
       "1  https://medium.com/policy/amendment-to-medium-...   \n",
       "2  https://medium.com/@aelcenganda/%E9%96%A9%E6%9...   \n",
       "3  https://medium.com/what-comes-to-mind/how-fast...   \n",
       "4  https://medium.com/what-comes-to-mind/a-game-f...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Medium Everyone’s stories and ideas Aug 13, 20...   \n",
       "1  Medium Everyone’s stories and ideas Aug 2, 201...   \n",
       "2  Yun-Chen Chien（簡韻真） Nobody in @g0v.tw, PM in s...   \n",
       "3  Vaibhav Khulbe Android App Developer | I write...   \n",
       "4  Vaibhav Khulbe Android App Developer | I write...   \n",
       "\n",
       "                               published  \\\n",
       "0  {'$date': '2012-08-13T22:54:53.510Z'}   \n",
       "1  {'$date': '2015-08-03T07:44:50.331Z'}   \n",
       "2  {'$date': '2017-02-05T13:08:17.410Z'}   \n",
       "3  {'$date': '2017-05-06T08:16:30.776Z'}   \n",
       "4  {'$date': '2017-06-04T14:46:25.772Z'}   \n",
       "\n",
       "                                               title  \\\n",
       "0   Medium Terms of Service – Medium Policy – Medium   \n",
       "1  Amendment to Medium Terms of Service Applicabl...   \n",
       "2  走入山與海之間：閩東大刀會和兩岸走私 – Yun-Chen Chien（簡韻真） – Medium   \n",
       "3  How fast can a camera get? – What comes to min...   \n",
       "4  A game for the lonely fox – What comes to mind...   \n",
       "\n",
       "                                              author  id  log_recommends  \n",
       "0  {'name': None, 'url': 'https://medium.com/@Med...   8         9.01201  \n",
       "1  {'name': None, 'url': 'https://medium.com/@Med...  14         3.49651  \n",
       "2  {'name': None, 'url': 'https://medium.com/@ael...  19         0.69315  \n",
       "3  {'name': None, 'url': 'https://medium.com/@vai...  22         1.38629  \n",
       "4  {'name': None, 'url': 'https://medium.com/@vai...  29         1.94591  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the columns date and time and preprocessing author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_to_time(publicate):\n",
    "    return publicate[11:-5]\n",
    "\n",
    "def pub_to_date(publicate):\n",
    "    return publicate[:10] \n",
    "\n",
    "def time_to_sec(time_s):\n",
    "    return ((int(time_s[:2])* 60 * 60) + (int(time_s[3:5])) * 60 + (int(time_s[6:])))\n",
    "\n",
    "# may be needed other division\n",
    "def sec_to_tofd(time_sec):\n",
    "    x = 3600\n",
    "    if(time_sec > 18 * x):\n",
    "        return 'E'\n",
    "    elif(time_sec > 12 * x):\n",
    "        return 'D'\n",
    "    elif(time_sec > 6 * x):\n",
    "        return 'M'\n",
    "    else:\n",
    "        return 'N'\n",
    "    \n",
    "def day_of_week(d):\n",
    "    days = date(int(d[:4]), int(d[5:7]), int(d[8:]))\n",
    "    return(days.weekday())\n",
    "\n",
    "def is_weekend(d):\n",
    "    if (d > 4):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def twitter_data(author_data):\n",
    "    return author_data['twitter']\n",
    "\n",
    "def url_data(author_data):\n",
    "    return author_data['url']\n",
    "\n",
    "def publish_to_str(publ):\n",
    "    return list(publ.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data['published'] = df_all_data['published'].map(publish_to_str)\n",
    "df_all_data['date'] = df_all_data['published'].map(pub_to_date)\n",
    "df_all_data['time'] = df_all_data['published'].map(pub_to_time)\n",
    "df_all_data['time_sec'] = df_all_data['time'].map(time_to_sec)\n",
    "df_all_data['time_of_day'] = df_all_data['time_sec'].map(sec_to_tofd)\n",
    "df_all_data['week_day'] = df_all_data['date'].map(day_of_week)\n",
    "df_all_data['weekend'] = df_all_data['week_day'].map(is_weekend)\n",
    "df_all_data['twitter'] = df_all_data['author'].map(twitter_data)\n",
    "df_all_data['url'] = df_all_data['author'].map(url_data)\n",
    "df_all_data['cont_len']= df_all_data['content'].map(len)\n",
    "df_all_data['title_len']= df_all_data['title'].map(len)\n",
    "\n",
    "df_all_test['published'] = df_all_test['published'].map(publish_to_str)\n",
    "df_all_test['date'] = df_all_test['published'].map(pub_to_date)\n",
    "df_all_test['time'] = df_all_test['published'].map(pub_to_time)\n",
    "df_all_test['time_sec'] = df_all_test['time'].map(time_to_sec)\n",
    "df_all_test['time_of_day'] = df_all_test['time_sec'].map(sec_to_tofd)\n",
    "df_all_test['week_day'] = df_all_test['date'].map(day_of_week)\n",
    "df_all_test['weekend'] = df_all_test['week_day'].map(is_weekend)\n",
    "df_all_test['twitter'] = df_all_test['author'].map(twitter_data)\n",
    "df_all_test['url'] = df_all_test['author'].map(url_data)\n",
    "df_all_test['cont_len']= df_all_test['content'].map(len)\n",
    "df_all_test['title_len']= df_all_test['title'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not here\n",
    "feats = ['time_of_day', 'week_day','cont_len', 'title_len', 'log_recommends']\n",
    "df = df_all_data[feats]\n",
    "df_test = df_all_test[feats[:-1]]\n",
    "#X_train, y_train = train_test_split(df, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>content</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>week_day</th>\n",
       "      <th>weekend</th>\n",
       "      <th>twitter</th>\n",
       "      <th>url</th>\n",
       "      <th>cont_len</th>\n",
       "      <th>title_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://medium.com/on-mornings/nocturnalmornin...</td>\n",
       "      <td>Member preview HITRECORD hitrecord.org is a ne...</td>\n",
       "      <td>2018-02-21T17:01:00.928Z</td>\n",
       "      <td>For Night Owls, the Day Starts with a Nocturna...</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@HIT...</td>\n",
       "      <td>2018-02-21</td>\n",
       "      <td>17:01:00</td>\n",
       "      <td>61260</td>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>@hitRECord</td>\n",
       "      <td>https://medium.com/@HITRECORD.org</td>\n",
       "      <td>12580</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://medium.com/wordsthatmatter/never-break...</td>\n",
       "      <td>Member preview Maria Bustillos is a journalist...</td>\n",
       "      <td>2017-12-07T15:56:00.967Z</td>\n",
       "      <td>Blockchain is Memory – Words That Matter – Medium</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@mar...</td>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>15:56:00</td>\n",
       "      <td>57360</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>@mariabustillos</td>\n",
       "      <td>https://medium.com/@mariabustillos</td>\n",
       "      <td>6759</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://medium.com/on-mornings/onmorningscredi...</td>\n",
       "      <td>HITRECORD hitrecord.org is a new kind of onlin...</td>\n",
       "      <td>2018-02-08T17:33:27.816Z</td>\n",
       "      <td>ON MORNINGS Credits – On Mornings – Medium</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@HIT...</td>\n",
       "      <td>2018-02-08</td>\n",
       "      <td>17:33:27</td>\n",
       "      <td>63207</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>@hitRECord</td>\n",
       "      <td>https://medium.com/@HITRECORD.org</td>\n",
       "      <td>7337</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://medium.com/@LanceUlanoff/apple-homepod...</td>\n",
       "      <td>Member preview Lance Ulanoff Tech expert, jour...</td>\n",
       "      <td>2018-02-09T18:04:12.262Z</td>\n",
       "      <td>Apple HomePod Review: Almost love – Lance Ulan...</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@Lan...</td>\n",
       "      <td>2018-02-09</td>\n",
       "      <td>18:04:12</td>\n",
       "      <td>65052</td>\n",
       "      <td>E</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>@LanceUlanoff</td>\n",
       "      <td>https://medium.com/@LanceUlanoff</td>\n",
       "      <td>13027</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://blog.medium.com/tips-and-tricks-for-me...</td>\n",
       "      <td>Medium Everyone’s stories and ideas Oct 2, 201...</td>\n",
       "      <td>2017-10-02T23:08:00.000Z</td>\n",
       "      <td>Tips and tricks for Medium writers – 3 min read</td>\n",
       "      <td>{'name': None, 'url': 'https://blog.medium.com...</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>23:08:00</td>\n",
       "      <td>83280</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>@Medium</td>\n",
       "      <td>https://blog.medium.com/@Medium</td>\n",
       "      <td>5723</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id  \\\n",
       "0  https://medium.com/on-mornings/nocturnalmornin...   \n",
       "1  https://medium.com/wordsthatmatter/never-break...   \n",
       "2  https://medium.com/on-mornings/onmorningscredi...   \n",
       "3  https://medium.com/@LanceUlanoff/apple-homepod...   \n",
       "4  https://blog.medium.com/tips-and-tricks-for-me...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Member preview HITRECORD hitrecord.org is a ne...   \n",
       "1  Member preview Maria Bustillos is a journalist...   \n",
       "2  HITRECORD hitrecord.org is a new kind of onlin...   \n",
       "3  Member preview Lance Ulanoff Tech expert, jour...   \n",
       "4  Medium Everyone’s stories and ideas Oct 2, 201...   \n",
       "\n",
       "                  published  \\\n",
       "0  2018-02-21T17:01:00.928Z   \n",
       "1  2017-12-07T15:56:00.967Z   \n",
       "2  2018-02-08T17:33:27.816Z   \n",
       "3  2018-02-09T18:04:12.262Z   \n",
       "4  2017-10-02T23:08:00.000Z   \n",
       "\n",
       "                                               title  \\\n",
       "0  For Night Owls, the Day Starts with a Nocturna...   \n",
       "1  Blockchain is Memory – Words That Matter – Medium   \n",
       "2         ON MORNINGS Credits – On Mornings – Medium   \n",
       "3  Apple HomePod Review: Almost love – Lance Ulan...   \n",
       "4    Tips and tricks for Medium writers – 3 min read   \n",
       "\n",
       "                                              author        date      time  \\\n",
       "0  {'name': None, 'url': 'https://medium.com/@HIT...  2018-02-21  17:01:00   \n",
       "1  {'name': None, 'url': 'https://medium.com/@mar...  2017-12-07  15:56:00   \n",
       "2  {'name': None, 'url': 'https://medium.com/@HIT...  2018-02-08  17:33:27   \n",
       "3  {'name': None, 'url': 'https://medium.com/@Lan...  2018-02-09  18:04:12   \n",
       "4  {'name': None, 'url': 'https://blog.medium.com...  2017-10-02  23:08:00   \n",
       "\n",
       "   time_sec time_of_day  week_day  weekend          twitter  \\\n",
       "0     61260           D         2    False       @hitRECord   \n",
       "1     57360           D         3    False  @mariabustillos   \n",
       "2     63207           D         3    False       @hitRECord   \n",
       "3     65052           E         4    False    @LanceUlanoff   \n",
       "4     83280           E         0    False          @Medium   \n",
       "\n",
       "                                  url  cont_len  title_len  \n",
       "0   https://medium.com/@HITRECORD.org     12580         55  \n",
       "1  https://medium.com/@mariabustillos      6759         49  \n",
       "2   https://medium.com/@HITRECORD.org      7337         42  \n",
       "3    https://medium.com/@LanceUlanoff     13027         58  \n",
       "4     https://blog.medium.com/@Medium      5723         47  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating 'Bag of authors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_set = df_all_data['url'].append(df_all_test['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "labels_authors = label_encoder.fit(auth_set)\n",
    "df_all_data['url'] = label_encoder.transform(df_all_data['url'])\n",
    "df_all_test['url'] = label_encoder.transform(df_all_test['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_encoded_cols_test = pd.DataFrame(onehot_encoder.fit_transform((df_all_test['url'].values).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-10d3faada135>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mauthor_encoded_categorical_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monehot_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_all_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2017\u001b[0m         \"\"\"\n\u001b[0;32m   2018\u001b[0m         return _transform_selected(X, self._fit_transform,\n\u001b[1;32m-> 2019\u001b[1;33m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[0;32m   2020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2021\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[1;34m(X, transform, selected, copy)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   2003\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactive_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2005\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "author_encoded_categorical_columns = pd.DataFrame(onehot_encoder.fit_transform((df_all_data['url'].values).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'author_encoded_categorical_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-2b4e26396122>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mauthor_encoded_cols_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthor_encoded_categorical_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'author_encoded_categorical_columns' is not defined"
     ]
    }
   ],
   "source": [
    "author_encoded_cols_test.shape, author_encoded_categorical_columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating 'Bag of time and week days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for i in feats[:2]:\n",
    "    df[i] = label_encoder.fit_transform(df[i])\n",
    "data_time_train = pd.DataFrame(onehot_encoder.fit_transform((df[feats[:2]])))\n",
    "data_train = data_time_train.join(df[feats[2:-1]])\n",
    "\n",
    "for i in feats[:2]:\n",
    "    df_test[i] = label_encoder.fit_transform(df_test[i])\n",
    "data_time_test = pd.DataFrame(onehot_encoder.fit_transform((df_test[feats[:2]])))\n",
    "data_test = data_time_train.join(df_test[feats[2:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>cont_len</th>\n",
       "      <th>title_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7007</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10817</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4588</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4436</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5474</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   10  cont_len  title_len\n",
       "0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0      7007         48\n",
       "1  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     10817         72\n",
       "2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0      4588         49\n",
       "3  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0      4436         56\n",
       "4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0      5474         55"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>cont_len</th>\n",
       "      <th>title_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12580.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6759.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7337.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13027.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5723.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   10  cont_len  title_len\n",
       "0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   12580.0       55.0\n",
       "1  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0    6759.0       49.0\n",
       "2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0    7337.0       42.0\n",
       "3  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   13027.0       58.0\n",
       "4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0    5723.0       47.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tf-Idf \n",
    "\n",
    "- with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "- with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2), max_features= 100000)\n",
    "df_counts_title = count_vect.fit_transform(df_all_data.title)\n",
    "\n",
    "tf_transformer_title = TfidfTransformer(use_idf=False).fit(df_counts_title)\n",
    "df_tf_title = tf_transformer_title.transform(df_counts_title)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "df_tfidf_title = tfidf_transformer.fit_transform(df_counts_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    801\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m             \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    804\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m             \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2), max_features= 100000)\n",
    "df_counts_content = count_vect.fit_transform(df_all_data.content)\n",
    "\n",
    "#tf_transformer_content = TfidfTransformer(use_idf=False).fit(df_counts_content)\n",
    "#df_tf_content = tf_transformer_content.transform(df_counts_content)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "df_tfidf_content = tfidf_transformer.fit_transform(df_counts_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2), max_features= 100000)\n",
    "df_counts_title_t = count_vect.fit_transform(df_all_test.title)\n",
    "\n",
    "tf_transformer_title = TfidfTransformer(use_idf=False).fit(df_counts_title_t)\n",
    "df_tf_title_te = tf_transformer_title.transform(df_counts_title_t)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "df_tfidf_title_te = tfidf_transformer.fit_transform(df_counts_title_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2), max_features= 100000)\n",
    "df_counts_content_t = count_vect.fit_transform(df_all_test.content)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "df_tfidf_content_te = tfidf_transformer.fit_transform(df_counts_content_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tfidf_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-59a7a0e80437>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_tfidf_content\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_tfidf_title\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthor_encoded_categorical_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_tfidf_content' is not defined"
     ]
    }
   ],
   "source": [
    "df_tfidf_content.shape, df_tfidf_title.shape, author_encoded_categorical_columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsing train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'author_encoded_categorical_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'author_encoded_categorical_columns' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_sparse = hstack([author_encoded_categorical_columns,\n",
    "                         df_tfidf_content, df_tfidf_title, data_train]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tfidf_content_te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_tfidf_content_te' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test_sparse = hstack([author_encoded_cols_test,\n",
    "                         df_tfidf_content_te, df_tfidf_title_te, data_test]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[feats[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse.shape, len(df[feats[-1]]), X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Split data on train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train_sparse, df[feats[-1]], random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * y_train.shape[0])\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid_sparse =  X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using different models and check MAE on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_err(arr1, arr2, p = 1):\n",
    "    if (len(arr1) == len(arr2)):\n",
    "        sum = 0\n",
    "        for i in range(len(arr1)):\n",
    "            sum += (abs(arr1[i] - arr2[i])) ** p\n",
    "        return ((sum ** (1 / p)) / len(arr1))\n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(3, 6):\n",
    "    ridge = Ridge(alpha = i / 4)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    print(\"alpha: \", i / 4, \" MAE: \", ridge.score(X_test, y_test), mean_err(ridge.predict(X_test), y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename,\n",
    "                          path_to_sample=os.path.join(PATH_TO_DATA, \n",
    "                                                      'sample_submission.csv')):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "    \n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(ridge_test_pred, os.path.join(PATH_TO_DATA,\n",
    "                                                    'assignment6_medium_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dirty Kaggle hacks. Form a submission file with all zeros. Make a submission. What do you get if you think about it? How is it going to help you with modifying your predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(np.zeros_like(ridge_test_pred), \n",
    "                      os.path.join(PATH_TO_DATA,\n",
    "                                   'medium_all_zeros_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify predictions in an appropriate way (based on your all-zero submission) and make a new submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_test_pred_modif = ridge_test_pred # You code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission_file(ridge_test_pred_modif, \n",
    "                      os.path.join(PATH_TO_DATA,\n",
    "                                   'assignment6_medium_submission_with_hack.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
