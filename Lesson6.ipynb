{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition on Kaggle: https://www.kaggle.com/c/how-good-is-your-medium-article/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading https://files.pythonhosted.org/packages/91/55/8cb23a97301b177e9c8e3226dba45bb454411de2cbd25746763267f226c2/tqdm-4.28.1-py2.py3-none-any.whl (45kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.28.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ' '.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_write(path_to_data,\n",
    "                               inp_filename, is_train=True):\n",
    "    ident = 0\n",
    "    features = ['_id', 'content', 'published', 'title', 'author']\n",
    "    prefix = 'train' if is_train else 'test'\n",
    "    feature_files = [open(os.path.join(path_to_data,\n",
    "                                       '{}_{}.txt'.format(prefix, feat)),\n",
    "                          'w', encoding=\"utf-8\")\n",
    "                     for feat in features]\n",
    " \n",
    "    with open(os.path.join(path_to_data, inp_filename), \n",
    "              encoding='utf-8') as inp_json_file:\n",
    "    \n",
    "        for line in tqdm_notebook(inp_json_file):\n",
    "            dict_data = {}\n",
    "            json_data = read_json_line(line)\n",
    "            for i in range(len(feature_files)):\n",
    "                dict_data[ident] = json_data[features[i]]\n",
    "                feature_files[i].write(str(dict_data) + '\\n')\n",
    "            ident += 1\n",
    "    for i in feature_files:\n",
    "        i.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'D:\\mlcourse\\Databases\\kaggle_medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4114576a65d84be9a7b80c6a86619d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_features_and_write(PATH_TO_DATA, 'train.json', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd97fe2409544775ab66d5dfd09f6211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_features_and_write(PATH_TO_DATA, 'test.json', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the following groups of features:\n",
    "\n",
    "- Tf-Idf with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "- Tf-Idf with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "- Time features: publication hour, whether it's morning, day, night, whether it's a weekend\n",
    "- Bag of authors (i.e. One-Hot-Encoded author names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extacting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'train'\n",
    "features = ['_id', 'content', 'published', 'title', 'author']\n",
    "\n",
    "l_files = [os.path.join(PATH_TO_DATA,\n",
    "                                       '{}_{}.txt'.format(prefix, feat))   \n",
    "                     for feat in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 31min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all_data = pd.DataFrame({})\n",
    "for k in range(0, len(features)):\n",
    "    f = codecs.open(l_files[k], \"r\", \"utf-8\")\n",
    "    arr = []\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        count += 1\n",
    "        arr.append(list(eval(line[:-2]).values())[0])\n",
    "    f.close()\n",
    "    ser_buf = pd.Series(arr)\n",
    "    df_all_data[features[k]] = ser_buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data['content'] = df_all_data['content'].map(strip_tags)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a popularity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:\\mlcourse\\Databases\\kaggle_medium/train_log1p_recommends.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data = df_all_data.join(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip content: \\xa0, \\u200b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division publications by hour, whether it's morning, day, night, whether it's a weekend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in df.index:\n",
    "    df_all_data['published'][i] = list(df_all_data['published'].values[i].values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the columns date and time and preprocessing author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_to_time(publicate):\n",
    "    return publicate[11:-5]\n",
    "\n",
    "def pub_to_date(publicate):\n",
    "    return publicate[:10] \n",
    "\n",
    "def time_to_sec(time_s):\n",
    "    return ((int(time_s[:2])* 60 * 60) + (int(time_s[3:5])) * 60 + (int(time_s[6:])))\n",
    "\n",
    "# may be needed other division\n",
    "def sec_to_tofd(time_sec):\n",
    "    x = 3600\n",
    "    if(time_sec > 18 * x):\n",
    "        return 'E'\n",
    "    elif(time_sec > 12 * x):\n",
    "        return 'D'\n",
    "    elif(time_sec > 6 * x):\n",
    "        return 'M'\n",
    "    else:\n",
    "        return 'N'\n",
    "    \n",
    "def day_of_week(d):\n",
    "    days = date(int(d[:4]), int(d[5:7]), int(d[8:]))\n",
    "    return(days.weekday())\n",
    "\n",
    "def is_weekend(d):\n",
    "    if (d > 4):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def rewrite_author(author):\n",
    "    return author['twitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data['date'] = df_all_data['published'].map(pub_to_date)\n",
    "df_all_data['time'] = df_all_data['published'].map(pub_to_time)\n",
    "df_all_data['time_sec'] = df_all_data['time'].map(time_to_sec)\n",
    "df_all_data['time_of_day'] = df_all_data['time_sec'].map(sec_to_tofd)\n",
    "df_all_data['week_day'] = df_all_data['date'].map(day_of_week)\n",
    "df_all_data['weekend'] = df_all_data['week_day'].map(is_weekend)\n",
    "df_all_data['twitter'] = df_all_data['author'].map(rewrite_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>content</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>log_recommends</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>week_day</th>\n",
       "      <th>weekend</th>\n",
       "      <th>twitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://medium.com/policy/medium-terms-of-serv...</td>\n",
       "      <td>Medium Everyone’s stories and ideas Aug 13, 20...</td>\n",
       "      <td>2012-08-13T22:54:53.510Z</td>\n",
       "      <td>Medium Terms of Service – Medium Policy – Medium</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@Med...</td>\n",
       "      <td>8</td>\n",
       "      <td>9.01201</td>\n",
       "      <td>2012-08-13</td>\n",
       "      <td>22:54:53</td>\n",
       "      <td>82493</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>@Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://medium.com/policy/amendment-to-medium-...</td>\n",
       "      <td>Medium Everyone’s stories and ideas Aug 2, 201...</td>\n",
       "      <td>2015-08-03T07:44:50.331Z</td>\n",
       "      <td>Amendment to Medium Terms of Service Applicabl...</td>\n",
       "      <td>{'name': None, 'url': 'https://medium.com/@Med...</td>\n",
       "      <td>14</td>\n",
       "      <td>3.49651</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>07:44:50</td>\n",
       "      <td>27890</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>@Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 _id  \\\n",
       "0  https://medium.com/policy/medium-terms-of-serv...   \n",
       "1  https://medium.com/policy/amendment-to-medium-...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Medium Everyone’s stories and ideas Aug 13, 20...   \n",
       "1  Medium Everyone’s stories and ideas Aug 2, 201...   \n",
       "\n",
       "                  published  \\\n",
       "0  2012-08-13T22:54:53.510Z   \n",
       "1  2015-08-03T07:44:50.331Z   \n",
       "\n",
       "                                               title  \\\n",
       "0   Medium Terms of Service – Medium Policy – Medium   \n",
       "1  Amendment to Medium Terms of Service Applicabl...   \n",
       "\n",
       "                                              author  id  log_recommends  \\\n",
       "0  {'name': None, 'url': 'https://medium.com/@Med...   8         9.01201   \n",
       "1  {'name': None, 'url': 'https://medium.com/@Med...  14         3.49651   \n",
       "\n",
       "         date      time  time_sec time_of_day  week_day  weekend  twitter  \n",
       "0  2012-08-13  22:54:53     82493           E         0    False  @Medium  \n",
       "1  2015-08-03  07:44:50     27890           M         0    False  @Medium  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not here\n",
    "feats = ['content', 'title', 'time_of_day', 'week_day', 'weekend', 'twitter', 'log_recommends']\n",
    "df = df_all_data[feats]\n",
    "X_train, y_train = train_test_split(df, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tf-Idf \n",
    "\n",
    "- with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "- with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2), max_features= 100000)\n",
    "df_counts_title = count_vect.fit_transform(df.title)\n",
    "\n",
    "tf_transformer_title = TfidfTransformer(use_idf=False).fit(df_counts_title)\n",
    "df_tf_title = tf_transformer_title.transform(df_counts_title)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "df_tfidf_title = tfidf_transformer.fit_transform(df_counts_title)\n",
    "df_tfidf_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 27min 30s\n",
      "Parser   : 191 ms"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2), max_features= 100000)\n",
    "df_counts_content = count_vect.fit_transform(df.content)\n",
    "\n",
    "tf_transformer_content = TfidfTransformer(use_idf=False).fit(df_counts_content)\n",
    "df_tf_content = tf_transformer_content.transform(df_counts_content)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "df_tfidf_content = tfidf_transformer.fit_transform(df_counts_content)\n",
    "df_tfidf_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "index not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-cb8445229a5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_extended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_tfidf_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_tfidf_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   6324\u001b[0m         \u001b[1;31m# For SparseDataFrame's benefit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6325\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[1;32m-> 6326\u001b[1;33m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[0;32m   6327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6328\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   6347\u001b[0m             \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6349\u001b[1;33m             \u001b[0mcan_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6351\u001b[0m             \u001b[1;31m# join indexes only using concat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   6347\u001b[0m             \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6349\u001b[1;33m             \u001b[0mcan_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6351\u001b[0m             \u001b[1;31m# join indexes only using concat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: index not found"
     ]
    }
   ],
   "source": [
    "df_extended = df.join(df_tfidf_content).join(df_tfidf_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))\n",
    "    \n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "     categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)    \n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, n_iter=5, random_state=42)),\n",
    "    ])\n",
    "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
